{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Emotion Recognition\n",
    "\n",
    "This file allows to predict the emotion of a certain image\n",
    "\n",
    "The program uses the haarcascade model (haarcascade_frontface_default.xml) to detect faces in an image. It later uses the CNN VGG16 to predict the emotion. \n",
    "\n",
    "There are 7 types of emotions it can predict: \n",
    "- Angry \n",
    "- Disgust\n",
    "- Fear\n",
    "- Happy \n",
    "- Sad\n",
    "- Surprise\n",
    "- Neutral\n",
    "\n",
    "Note: In order for the model to detect the face(s), all parts of the face must be visible. Additionally, it is preferred that the face is directly looking at the camera, with minimal tilt or angle. Covering the face with hands or titling the face so that all parts of the face are not visible may not allow the model to detect the face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries needed to be installed for this project (pip install <library>):\n",
    "- tensorflow\n",
    "- opencv-python\n",
    "- ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import numpy\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting constants and file names\n",
    "MODEL_NAME = 'vgg16' # using the VGG16 model\n",
    "MODEL_FILE = 'fer_' + MODEL_NAME\n",
    "\n",
    "MODEL_FILE_H5 = MODEL_FILE + '.h5'\n",
    "MODEL_FILE_JSON = MODEL_FILE + '.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = r'/Users/fazalmittu/downloads/sample_predict'  # change this to your personal directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates user_test folder\n",
    "USER_TEST_PATH = os.path.join(BASE_DIR, 'user_test')\n",
    "if not(os.path.exists(os.path.join(BASE_DIR, 'user_test'))):\n",
    "    os.mkdir(USER_TEST_PATH)\n",
    "    print('Directory created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please upload your picture to the following directory: \n",
      "/Users/ruhiy/CS/Machine Learning Projects/FER/sample_predict/user_test\n",
      "You have succesfully uploaded the picture: sad_face.jpg\n"
     ]
    }
   ],
   "source": [
    "# asks user to upload picture to /sample_predict/user_test/\n",
    "print('Please upload your picture to the following directory: \\n' + USER_TEST_PATH)\n",
    "# asks for file name of the picture\n",
    "img_file_name = input(\"Please enter the file name of the test image (ex. 'happy_face.jpg')\")\n",
    "if os.path.isfile(os.path.join(USER_TEST_PATH, img_file_name)):\n",
    "    # checks if the picture is in the folder\n",
    "    print('You have succesfully uploaded the picture:', img_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-07 11:01:05.272413: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# loading the model\n",
    "json_file = open(MODEL_FILE_JSON, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(MODEL_FILE_H5)\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting image resizing parameters\n",
    "WIDTH = 48\n",
    "HEIGHT = 48\n",
    "x=None\n",
    "y=None\n",
    "# setting labels\n",
    "labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sad_face.jpg'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Loaded\n"
     ]
    }
   ],
   "source": [
    "# loading image\n",
    "full_size_image = cv2.imread(os.path.join(USER_TEST_PATH, img_file_name))\n",
    "print(\"Image Loaded\")\n",
    "\n",
    "# detecting face(s)\n",
    "gray=cv2.cvtColor(full_size_image,cv2.COLOR_RGB2GRAY)\n",
    "face = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "faces = face.detectMultiScale(gray, 1.3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[84 56 89 89]]\n"
     ]
    }
   ],
   "source": [
    "print(faces) # prints coordinates of bounding box for each face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-07 11:01:11.954703: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Emotion: Fear\n"
     ]
    }
   ],
   "source": [
    "# detecting emotion\n",
    "for (x, y, w, h) in faces:\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n",
    "        \n",
    "        if MODEL_NAME == 'vgg16': # convert 1d image to 3d for vgg16\n",
    "            cropped_img = np.insert(cropped_img,1, 0 ,axis = 3)\n",
    "            cropped_img = np.insert(cropped_img,2, 0 ,axis = 3)\n",
    "            \n",
    "        cv2.normalize(cropped_img, cropped_img, alpha=0, beta=1, norm_type=cv2.NORM_L2, dtype=cv2.CV_32F)\n",
    "        cv2.rectangle(full_size_image, (x, y), (x + w, y + h), (0, 255, 0), 1)\n",
    "        # predicting the emotion\n",
    "        yhat= loaded_model.predict(cropped_img)\n",
    "        cv2.putText(full_size_image, labels[int(np.argmax(yhat))], (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "        print(\"Emotion: \"+labels[int(np.argmax(yhat))])\n",
    "\n",
    "cv2.imshow('Emotion', full_size_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw bounding box for faces\n",
    "def draw_bounding_box(face_coordinates, image_array, color):\n",
    "    x, y, w, h = face_coordinates\n",
    "    cv2.rectangle(image_array, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "for face in faces:\n",
    "    draw_bounding_box(face, gray, (0, 255, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb_image = cv2.cvtColor(full_size_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# store image with bounding box in /user_test/labeled/\n",
    "if not(os.path.exists(os.path.join(USER_TEST_PATH, 'labeled'))):\n",
    "    os.mkdir(os.path.join(USER_TEST_PATH, 'labeled'))\n",
    "\n",
    "cv2.imwrite(os.path.join(USER_TEST_PATH, 'labeled', img_file_name), bgr_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}