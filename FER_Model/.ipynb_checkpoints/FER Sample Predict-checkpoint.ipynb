{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e67fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import numpy\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22c81061",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'vgg16'\n",
    "MODEL_FILE = 'fer_' + MODEL_NAME\n",
    "\n",
    "MODEL_FILE_H5 = MODEL_FILE + '.h5'\n",
    "MODEL_FILE_JSON = MODEL_FILE + '.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfc0680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = r'C:\\Users\\ruhiy\\Documents\\Machine Learning\\Facial Recognition\\fer-main\\sample_predict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f4861d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates user_test folder\n",
    "USER_TEST_PATH = os.path.join(BASE_DIR, 'user_test')\n",
    "if not(os.path.exists(os.path.join(BASE_DIR, 'user_test'))):\n",
    "    os.mkdir(user_test_path)\n",
    "    print('Directory created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1da44a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please upload your picture to the following directory: \n",
      "C:\\Users\\ruhiy\\Documents\\Machine Learning\\Facial Recognition\\fer-main\\sample_predict\\user_test\n",
      "Please enter the file name of the test image (ex. 'faces.jpg')happy_face.jpg\n",
      "You have succesfully uploaded the picture: happy_face.jpg\n"
     ]
    }
   ],
   "source": [
    "print('Please upload your picture to the following directory: \\n' + USER_TEST_PATH)\n",
    "img_file_name = input(\"Please enter the file name of the test image (ex. 'faces.jpg')\")\n",
    "if os.path.isfile(os.path.join(user_test_path, img_file_name)):\n",
    "    print('You have succesfully uploaded the picture:', img_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4b3889c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# loading the model\n",
    "json_file = open(MODEL_FILE_JSON, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(MODEL_FILE_H5)\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a42a982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting image resizing parameters\n",
    "WIDTH = 48\n",
    "HEIGHT = 48\n",
    "x=None\n",
    "y=None\n",
    "labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eeae19ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Loaded\n"
     ]
    }
   ],
   "source": [
    "# loading image\n",
    "full_size_image = cv2.imread(os.path.join(USER_TEST_PATH, img_file_name))\n",
    "print(\"Image Loaded\")\n",
    "gray=cv2.cvtColor(full_size_image,cv2.COLOR_RGB2GRAY)\n",
    "face = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "faces = face.detectMultiScale(gray, 1.3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2976b74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[257 161 614 614]]\n"
     ]
    }
   ],
   "source": [
    "print(faces) # prints coordinates of bounding box for each face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0bdbc000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion: Happy\n"
     ]
    }
   ],
   "source": [
    "# detecting faces\n",
    "for (x, y, w, h) in faces:\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n",
    "        \n",
    "        if MODEL_NAME == 'vgg16': # convert 1d image to 3d for vgg16\n",
    "            cropped_img = np.insert(cropped_img,1, 0 ,axis = 3)\n",
    "            cropped_img = np.insert(cropped_img,2, 0 ,axis = 3)\n",
    "            \n",
    "        cv2.normalize(cropped_img, cropped_img, alpha=0, beta=1, norm_type=cv2.NORM_L2, dtype=cv2.CV_32F)\n",
    "        cv2.rectangle(full_size_image, (x, y), (x + w, y + h), (0, 255, 0), 1)\n",
    "        # predicting the emotion\n",
    "        yhat= loaded_model.predict(cropped_img)\n",
    "        cv2.putText(full_size_image, labels[int(np.argmax(yhat))], (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "        print(\"Emotion: \"+labels[int(np.argmax(yhat))])\n",
    "\n",
    "cv2.imshow('Emotion', full_size_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0625c1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_box(face_coordinates, image_array, color):\n",
    "    x, y, w, h = face_coordinates\n",
    "    cv2.rectangle(image_array, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "for face in faces:\n",
    "    draw_bounding_box(face, gray, (0, 255, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fcda2e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb_image = cv2.cvtColor(full_size_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "if not(os.path.exists(os.path.join(USER_TEST_PATH, 'labeled'))):\n",
    "    os.mkdir(os.path.join(USER_TEST_PATH, 'labeled'))\n",
    "\n",
    "cv2.imwrite(os.path.join(USER_TEST_PATH, 'labeled', img_file_name), bgr_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab6e033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fer_cnn",
   "language": "python",
   "name": "fer_cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
